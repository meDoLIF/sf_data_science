{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\medol\\AppData\\Local\\Temp\\ipykernel_2016\\3730758636.py:25: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn')\n"
     ]
    }
   ],
   "source": [
    "#импорт библиотек\n",
    "import numpy as np #для матричных вычислений\n",
    "import pandas as pd #для анализа и предобработки данных\n",
    "import matplotlib.pyplot as plt #для визуализации\n",
    "import seaborn as sns #для визуализации\n",
    "\n",
    "from sklearn import linear_model #линейные моделиё\n",
    "from sklearn import tree #деревья решений\n",
    "from sklearn import ensemble #ансамбли\n",
    "from sklearn import metrics #метрики\n",
    "from sklearn import preprocessing #предобработка\n",
    "from sklearn.model_selection import train_test_split #сплитование выборки\n",
    "from sklearn.model_selection import cross_val_score #Кросс валидация\n",
    "\n",
    "# Импорт оптимизаторов параметров\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import hyperopt\n",
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "\n",
    "import optuna\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо обучить две модели: логистическую регрессию и случайный лес. Далее нужно сделать подбор гиперпараметров с помощью базовых и продвинутых методов оптимизации. Важно использовать все четыре метода (GridSeachCV, RandomizedSearchCV, Hyperopt, Optuna) хотя бы по разу, максимальное количество итераций не должно превышать 50.\n",
    "\n",
    "В качестве метрики будем использовать F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/_train_sem09 (1).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Activity</th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "      <th>D6</th>\n",
       "      <th>D7</th>\n",
       "      <th>D8</th>\n",
       "      <th>D9</th>\n",
       "      <th>...</th>\n",
       "      <th>D1767</th>\n",
       "      <th>D1768</th>\n",
       "      <th>D1769</th>\n",
       "      <th>D1770</th>\n",
       "      <th>D1771</th>\n",
       "      <th>D1772</th>\n",
       "      <th>D1773</th>\n",
       "      <th>D1774</th>\n",
       "      <th>D1775</th>\n",
       "      <th>D1776</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.497009</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.132956</td>\n",
       "      <td>0.678031</td>\n",
       "      <td>0.273166</td>\n",
       "      <td>0.585445</td>\n",
       "      <td>0.743663</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.606291</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111209</td>\n",
       "      <td>0.803455</td>\n",
       "      <td>0.106105</td>\n",
       "      <td>0.411754</td>\n",
       "      <td>0.836582</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.033300</td>\n",
       "      <td>0.480124</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.209791</td>\n",
       "      <td>0.610350</td>\n",
       "      <td>0.356453</td>\n",
       "      <td>0.517720</td>\n",
       "      <td>0.679051</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538825</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.196344</td>\n",
       "      <td>0.724230</td>\n",
       "      <td>0.235606</td>\n",
       "      <td>0.288764</td>\n",
       "      <td>0.805110</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.517794</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.494734</td>\n",
       "      <td>0.781422</td>\n",
       "      <td>0.154361</td>\n",
       "      <td>0.303809</td>\n",
       "      <td>0.812646</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1777 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Activity        D1        D2    D3   D4        D5        D6        D7  \\\n",
       "0         1  0.000000  0.497009  0.10  0.0  0.132956  0.678031  0.273166   \n",
       "1         1  0.366667  0.606291  0.05  0.0  0.111209  0.803455  0.106105   \n",
       "2         1  0.033300  0.480124  0.00  0.0  0.209791  0.610350  0.356453   \n",
       "3         1  0.000000  0.538825  0.00  0.5  0.196344  0.724230  0.235606   \n",
       "4         0  0.100000  0.517794  0.00  0.0  0.494734  0.781422  0.154361   \n",
       "\n",
       "         D8        D9  ...  D1767  D1768  D1769  D1770  D1771  D1772  D1773  \\\n",
       "0  0.585445  0.743663  ...      0      0      0      0      0      0      0   \n",
       "1  0.411754  0.836582  ...      1      1      1      1      0      1      0   \n",
       "2  0.517720  0.679051  ...      0      0      0      0      0      0      0   \n",
       "3  0.288764  0.805110  ...      0      0      0      0      0      0      0   \n",
       "4  0.303809  0.812646  ...      0      0      0      0      0      0      0   \n",
       "\n",
       "   D1774  D1775  D1776  \n",
       "0      0      0      0  \n",
       "1      0      1      0  \n",
       "2      0      0      0  \n",
       "3      0      0      0  \n",
       "4      0      0      0  \n",
       "\n",
       "[5 rows x 1777 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#проверим что датасет сформировался корректно\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Первый столбец Activity содержит экспериментальные данные, описывающие фактический биологический ответ [0, 1]; (Целевой признак)\n",
    "* Остальные столбцы D1-D1776 представляют собой молекулярные дескрипторы — это вычисляемые свойства, которые могут фиксировать некоторые характеристики молекулы, например размер, форму или состав элементов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер df (3751, 1777)\n",
      "Размер X (3751, 1776)\n",
      "Размер y (3751,)\n"
     ]
    }
   ],
   "source": [
    "# Разобъём наши данные на Зависимые (y) и Независимые (X)\n",
    "\n",
    "X = df.drop(['Activity'], axis = 1)\n",
    "y = df['Activity']\n",
    "\n",
    "#Проверим по размерам выборок что разделение прошло успешно\n",
    "\n",
    "print('Размер df', df.shape)\n",
    "print('Размер X', X.shape)\n",
    "print('Размер y', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.542255\n",
       "0    0.457745\n",
       "Name: Activity, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Проверим, насколько равномерно разделены целевые данные, чтобы определить необходимость в стратификации при разделении на тренировочную и тестовую выборки\n",
    "\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные разделены примерно поровну, поэтому делать стратификацию не обязательно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Разделим данные на тренировочные и тестовые\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state= 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание BaseLine моделей\n",
    "\n",
    "Высчитаем F1-score для моделей Логистической регрессии и Случайном лесе на параметрах по умолчанию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#Baseline для логистической регрессии\n",
    "logReg_base = linear_model.LogisticRegression(random_state=42, max_iter= 50)\n",
    "\n",
    "logReg_base.fit(X_train, y_train)\n",
    "\n",
    "y_pred_logReg_base = logReg_base.predict(X_test)\n",
    "\n",
    "f1_logReg_base = metrics.f1_score(y_test, y_pred_logReg_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score на Baseline для логистической регресии равен 0.79\n"
     ]
    }
   ],
   "source": [
    "print('F1-score на Baseline для логистической регресии равен {:.2f}'.format(f1_logReg_base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline для случайного леса\n",
    "\n",
    "rf_base = ensemble.RandomForestClassifier(random_state= 42)\n",
    "\n",
    "rf_base.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf_base = rf_base.predict(X_test)\n",
    "\n",
    "f1_rf_base = metrics.f1_score(y_test, y_pred_rf_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score на Baseline для случайного леса равен 0.83\n"
     ]
    }
   ],
   "source": [
    "print('F1-score на Baseline для случайного леса равен {:.2f}'.format(f1_rf_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Построение модели с помощью GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На основе логистической регресии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 10.1 s\n",
      "Wall time: 7min 30s\n"
     ]
    }
   ],
   "source": [
    "param_grid = [\n",
    "    {'penalty' : ['l2', 'none'], # тип регуляризации\n",
    "    'solver' : ['newton-cg', 'lbfgs', 'sag'], # алгоритм оптимизации\n",
    "    'C' : [0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 1]}, # уровень силы регурялизации\n",
    "    #поскольку разные алгоритмы поддерживают разные типы регуляризации мы создадим еще 1 набор параметров\n",
    "    \n",
    "    {'penalty': ['l1', 'l2'] ,\n",
    "    'solver': ['liblinear', 'saga'],\n",
    "    'C': [0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 1]}\n",
    "]\n",
    "\n",
    "gs_logReg = GridSearchCV(\n",
    "    estimator = linear_model.LogisticRegression(random_state= 42, max_iter = 50),\n",
    "    param_grid = param_grid,\n",
    "    cv = 5,\n",
    "    n_jobs = -1,\n",
    "    scoring='f1'\n",
    ")\n",
    "\n",
    "%time gs_logReg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_gs_logReg = gs_logReg.predict(X_test)\n",
    "\n",
    "f1_gs_logReg = metrics.f1_score(y_test, y_pred_gs_logReg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наилучшие значения гиперпараметров: {'C': 0.1, 'penalty': 'l2', 'solver': 'newton-cg'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Наилучшие значения гиперпараметров: {}\".format(gs_logReg.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# отрисуем, как менялась точность при различных гиперпараметрах\n",
    "visual = pd.pivot_table(pd.DataFrame(gs_logReg.cv_results_),\n",
    "                        values='mean_test_score', index='param_C',\n",
    "                        columns='param_solver')\n",
    "sns.heatmap(visual)\n",
    "plt.title('Тепловая карта зависимости метрики accuracy от solver и С') # подпись графика\n",
    "sns.set(rc={'figure.figsize':(12, 8)}) #задаем размер графика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из тепловой карты видно, что нет смысла пробовать создать модель с большей силой регуляризации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score на Baseline для логистической регресии равен 0.79\n",
      "F1-score на GridSearchCV для логистической регресии равен 0.79\n"
     ]
    }
   ],
   "source": [
    "#Сравним результат с BaseLine\n",
    "print('F1-score на Baseline для логистической регресии равен {:.2f}'.format(f1_logReg_base))\n",
    "print('F1-score на GridSearchCV для логистической регресии равен {:.2f}'.format(f1_gs_logReg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К сожалению получить показатель лучше чем BaseLine не удалось"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На основе Случайного Леса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 34 s\n",
      "Wall time: 25min 3s\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'min_samples_leaf' : list(np.linspace(5, 25, 10, dtype=int)),\n",
    "    'max_depth' : list(np.linspace(1, 30, 6, dtype = int)),\n",
    "    'n_estimators' : list(np.linspace(80,200, 30, dtype = int))\n",
    "}\n",
    "\n",
    "gs_rf = GridSearchCV(\n",
    "    estimator= ensemble.RandomForestClassifier(random_state= 42),\n",
    "    param_grid= param_grid,\n",
    "    cv = 5,\n",
    "    n_jobs= -1,\n",
    "    scoring = 'f1'\n",
    ")\n",
    "\n",
    "%time gs_rf.fit(X_train, y_train)\n",
    "\n",
    "f1_gs_rf = gs_rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наилучшие значения гиперпараметров: {'max_depth': 30, 'min_samples_leaf': 5, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "print(\"Наилучшие значения гиперпараметров: {}\".format(gs_rf.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score на Baseline для случайного леса равен 0.83\n",
      "F1-score на GridResearchCV для случайного леса равен 0.83\n"
     ]
    }
   ],
   "source": [
    "print('F1-score на Baseline для случайного леса равен {:.2f}'.format(f1_rf_base))\n",
    "print('F1-score на GridResearchCV для случайного леса равен {:.2f}'.format(f1_gs_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К сожалению получить показатель лучше чем BaseLine не удалось"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.69 s\n",
      "Wall time: 3min 13s\n"
     ]
    }
   ],
   "source": [
    "param_distribution = [\n",
    "    {'penalty' : ['l2', 'none'], # тип регуляризации\n",
    "    'solver' : ['newton-cg', 'lbfgs', 'sag'], # алгоритм оптимизации\n",
    "    'C': list(np.linspace(0.01, 1, 10, dtype=float))}, # уровень силы регурялизации\n",
    "    #поскольку разные алгоритмы поддерживают разные типы регуляризации мы создадим еще 1 набор параметров\n",
    "    \n",
    "    {'penalty': ['l1', 'l2'] ,\n",
    "    'solver': ['liblinear', 'saga'],\n",
    "    'C': list(np.linspace(0.01, 1, 10, dtype=float))}\n",
    "]\n",
    "\n",
    "rs_logReg = RandomizedSearchCV(\n",
    "    estimator= linear_model.LogisticRegression(random_state=42, max_iter=50),\n",
    "    param_distributions=param_distribution,\n",
    "    cv = 5,\n",
    "    n_iter= 50,\n",
    "    n_jobs= -1,\n",
    "    scoring='f1'\n",
    ")\n",
    "\n",
    "%time rs_logReg.fit(X_train, y_train)\n",
    "\n",
    "f1_rs_logReg = rs_logReg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наилучшие значения гиперпараметров: {'solver': 'liblinear', 'penalty': 'l1', 'C': 0.56}\n"
     ]
    }
   ],
   "source": [
    "print(\"Наилучшие значения гиперпараметров: {}\".format(rs_logReg.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score на Baseline для логистической регресии равен 0.79\n",
      "F1-score на RandomizedSrearchCV для логистической регресии равен 0.80\n"
     ]
    }
   ],
   "source": [
    "#Сравним результат с BaseLine\n",
    "print('F1-score на Baseline для логистической регресии равен {:.2f}'.format(f1_logReg_base))\n",
    "print('F1-score на RandomizedSrearchCV для логистической регресии равен {:.2f}'.format(f1_rs_logReg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Случайный лес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distribution = {\n",
    "    'min_samples_leaf' : list(np.linspace(5, 25, 10, dtype=int)),\n",
    "    'max_depth' : list(np.linspace(1, 30, 6, dtype = int)),\n",
    "    'n_estimators' : list(np.linspace(80,200, 30, dtype = int))\n",
    "}\n",
    "\n",
    "rs_rf = RandomizedSearchCV(\n",
    "    estimator=ensemble.RandomForestClassifier(random_state=42),\n",
    "    param_distributions=param_distribution,\n",
    "    cv = 5,\n",
    "    n_iter = 50,\n",
    "    n_jobs= -1,\n",
    "    scoring= 'f1'\n",
    ")\n",
    "\n",
    "rs_rf.fit(X_train, y_train)\n",
    "\n",
    "f1_rs_rf = rs_rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наилучшие значения гиперпараметров: {'n_estimators': 183, 'min_samples_leaf': 5, 'max_depth': 24}\n"
     ]
    }
   ],
   "source": [
    "print(\"Наилучшие значения гиперпараметров: {}\".format(rs_rf.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score на Baseline для случайного леса равен 0.83\n",
      "F1-score на RandomizedSrearchCV для случайного леса равен 0.83\n"
     ]
    }
   ],
   "source": [
    "#Сравним результат с BaseLine\n",
    "print('F1-score на Baseline для случайного леса равен {:.2f}'.format(f1_rf_base))\n",
    "print('F1-score на RandomizedSrearchCV для случайного леса равен {:.2f}'.format(f1_rs_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperOpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"space = [\\n    {'penalty' : hp.choice(label='penalty', options= ['l2', 'none']), # тип регуляризации\\n    'solver' : hp.choice(label = 'solver', options= ['newton-cg', 'lbfgs', 'sag']), # алгоритм оптимизации\\n    'C' : hp.loguniform('C', low = -2*np.log(10), high = 1)}, # уровень силы регурялизации\\n    #поскольку разные алгоритмы поддерживают разные типы регуляризации мы создадим еще 1 набор параметров\\n    \\n    {'penalty': hp.choice(label='penalty', options= ['l1', 'l2']) ,\\n    'solver': hp.choice(label = 'solver', options= ['liblinear', 'saga']),\\n    'C': hp.loguniform('C', low = -2*np.log(10), high = 1)}\\n]\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#зададим пространство поиска гиперпараметров\n",
    "\"\"\"space = [\n",
    "    {'penalty' : hp.choice(label='penalty', options= ['l2', 'none']), # тип регуляризации\n",
    "    'solver' : hp.choice(label = 'solver', options= ['newton-cg', 'lbfgs', 'sag']), # алгоритм оптимизации\n",
    "    'C' : hp.loguniform('C', low = -2*np.log(10), high = 1)}, # уровень силы регурялизации\n",
    "    #поскольку разные алгоритмы поддерживают разные типы регуляризации мы создадим еще 1 набор параметров\n",
    "    \n",
    "    {'penalty': hp.choice(label='penalty', options= ['l1', 'l2']) ,\n",
    "    'solver': hp.choice(label = 'solver', options= ['liblinear', 'saga']),\n",
    "    'C': hp.loguniform('C', low = -2*np.log(10), high = 1)}\n",
    "]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Версия Hyperopt : 0.2.7\n"
     ]
    }
   ],
   "source": [
    "print(\"Версия Hyperopt : {}\".format(hyperopt.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "space ={\n",
    "    'penalty' : hp.choice(label='penalty', options= ['l2', 'none']), # тип регуляризации\n",
    "    'solver' : hp.choice(label = 'solver', options= ['newton-cg', 'lbfgs', 'sag']), # алгоритм оптимизации\n",
    "    'C' : hp.loguniform(label='C', low=-2*np.log(10), high=2*np.log(10))}, # уровень силы регурялизации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random_state = 42\n",
    "\n",
    "def hyperopt_lr(params, cv = 5, X = X_train, y = y_train, random_state = random_state):\n",
    "    params = {\n",
    "        'penalty' : hp.choice(label='penalty', options= ['l2', 'none']), # тип регуляризации\n",
    "        'solver' : hp.choice(label = 'solver', options= ['newton-cg', 'lbfgs', 'sag']), # алгоритм оптимизации\n",
    "        'C' : hp.loguniform(label='C', low=-2*np.log(10), high=2*np.log(10))\n",
    "    }\n",
    "    \n",
    "    model = linear_model.LogisticRegression(**params, random_state = random_state)\n",
    "    \n",
    "    model.fit(X, y)\n",
    "    \n",
    "    score = cross_val_score(model, X, y, cv=cv, scoring=\"f1\", n_jobs=-1).mean()\n",
    "    \n",
    "    return -score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%time\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "best = fmin(hyperopt_lr, #наша функция\n",
    "    space = space, #Пространство параметров\n",
    "    max_evals= 50, #Количество итераций\n",
    "    trials = trials,# логирование результатов\n",
    "    rstate = np.random.default_rng(random_state)\n",
    ")\n",
    "\n",
    "print('Наилучшие параметры {}'.format(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "100%|██████████| 50/50 [06:17<00:00,  7.56s/trial, best loss: -0.7916411179230474]\n",
      "Наилучшие значения гиперпараметров {'C': 0.028670415126948956, 'penalty': 0, 'solver': 0}\n"
     ]
    }
   ],
   "source": [
    "# зададим пространство поиска гиперпараметров - получится 3 набора, так как для каждого типа регуляции подходят только определённые алгоритмы оптимизации\n",
    "space = {'penalty': hp.choice(label='penalty', options=['l2', 'none']) , # тип регуляризации\n",
    "              'solver': hp.choice(label='solver', options=['lbfgs', 'sag', 'newton-cg']), # алгоритм оптимизации\n",
    "              'C': hp.uniform('C', 0.01, 1)\n",
    "              } # алгоритм оптимизации\n",
    "\n",
    "# зафиксируем random_state\n",
    "random_state = 42\n",
    "\n",
    "def hyperopt(space, cv=5, X=X, y=y, random_state=random_state):\n",
    "    \"\"\" Функция, обучающая модель LogisticRegression\n",
    "    по переданным гиперпараметрам\n",
    "\n",
    "    Args:\n",
    "        space (dict): набор гиперпараметров\n",
    "        cv (int, optional=5): Количество холдов кросс-валидации. Defaults to 5.\n",
    "        X (DataFrame): DataFrame с признаками. Defaults to X_train.\n",
    "        y (Series): Series с целевым признаком. Defaults to y_train.\n",
    "        random_state (int): Рандомное число для воспроизводимости результата. Defaults to random_state.\n",
    "\n",
    "    Returns:\n",
    "        score(float): метрика F1\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model=linear_model.LogisticRegression(\n",
    "        penalty = space['penalty'],\n",
    "        solver = space['solver'],\n",
    "        C=np.abs(float(space['C'])),\n",
    "        l1_ratio=float(space['l1_ratio']),\n",
    "        random_state=random_state,\n",
    "        max_iter=50        \n",
    "    )        \n",
    "    except KeyError:\n",
    "          \n",
    "        model=linear_model.LogisticRegression(\n",
    "        penalty = space['penalty'],\n",
    "        solver = space['solver'],\n",
    "        C=np.abs(float(space['C'])),\n",
    "        random_state=random_state,\n",
    "        max_iter=50\n",
    "           )\n",
    "        \n",
    "     \n",
    "    # применим  cross validation \n",
    "    score = cross_val_score(model, X, y, cv=cv, scoring=\"f1\", n_jobs=-1).mean()\n",
    "    # метрику необходимо минимизировать, поэтому ставим знак минус\n",
    "    return -score\n",
    "\n",
    "\n",
    "\n",
    "%time\n",
    "              \n",
    "trials = Trials() # используется для логирования результатов  \n",
    "\n",
    "best=fmin(hyperopt, # наша функция \n",
    "          space=space, # пространство гиперпараметров\n",
    "          algo=tpe.suggest, # алгоритм оптимизации, установлен по умолчанию, задавать необязательно\n",
    "          max_evals=50, # максимальное количество итераций\n",
    "          trials=trials, # логирование результатов\n",
    "          rstate=np.random.default_rng(random_state)# фиксируем для повторяемости результата\n",
    "         )\n",
    "print(f'Наилучшие значения гиперпараметров {best}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.028670415126948956, 'penalty': 'l2', 'solver': 'lbfgs'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hyperopt import space_eval\n",
    "hyperparams = space_eval(space, best)\n",
    "hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# рассчитаем точность для тестовой выборки\n",
    "model = linear_model.LogisticRegression(\n",
    "    random_state=random_state, \n",
    "    penalty=hyperparams['penalty'],\n",
    "    solver=hyperparams['solver'],\n",
    "    C=hyperparams['C']\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_ho_lr = model.predict(X_test)\n",
    "\n",
    "f1_ho_lr = metrics.f1_score(y_test, y_pred_ho_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f1_logReg_base' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#Сравним результат с BaseLine\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mF1-score на Baseline для логистической регресии равен \u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(f1_logReg_base))\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mF1-score на HyperOpt для логистической регресии равен \u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(f1_ho_lr))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'f1_logReg_base' is not defined"
     ]
    }
   ],
   "source": [
    "#Сравним результат с BaseLine\n",
    "print('F1-score на Baseline для логистической регресии равен {:.2f}'.format(f1_logReg_base))\n",
    "print('F1-score на HyperOpt для логистической регресии равен {:.2f}'.format(f1_ho_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Случайный лес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "space={'n_estimators': hp.quniform('n_estimators', 80, 200, 1),\n",
    "       'max_depth' : hp.quniform('max_depth', 1, 30, 1),\n",
    "       'min_samples_leaf': hp.quniform('min_samples_leaf', 5, 25, 1)\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_rf(params, cv=5, X=X_train, y=y_train, random_state=random_state):\n",
    "    \n",
    "    params = {'n_estimators': int(params['n_estimators']), \n",
    "              'max_depth': int(params['max_depth']), \n",
    "             'min_samples_leaf': int(params['min_samples_leaf'])\n",
    "    }\n",
    "    # используем эту комбинацию для построения модели   \n",
    "    model = ensemble.RandomForestClassifier(**params, random_state=random_state)\n",
    "\n",
    "    # обучаем модель\n",
    "    model.fit(X, y)\n",
    "    score = cross_val_score(model, X, y, cv=cv, scoring=\"f1\", n_jobs=-1).mean()\n",
    "    \n",
    "    return -score    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "100%|██████████| 50/50 [02:09<00:00,  2.59s/trial, best loss: -0.8051423755259064]\n",
      "Наилучшие значения гиперпараметров {'max_depth': 28.0, 'min_samples_leaf': 5.0, 'n_estimators': 101.0}\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "trials = Trials() # используется для логирования результатов\n",
    "\n",
    "best=fmin(hyperopt_rf, # наша функция \n",
    "          space=space, # пространство гиперпараметров\n",
    "          algo=tpe.suggest, # алгоритм оптимизации, установлен по умолчанию, задавать необязательно\n",
    "          max_evals=50, # максимальное количество итераций\n",
    "          trials=trials, # логирование результатов\n",
    "          rstate=np.random.default_rng(random_state)# фиксируем для повторяемости результата\n",
    "         )\n",
    "print(\"Наилучшие значения гиперпараметров {}\".format(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# рассчитаем точность для тестовой выборки\n",
    "model = ensemble.RandomForestClassifier(\n",
    "    random_state=random_state, \n",
    "    n_estimators=int(best['n_estimators']),\n",
    "    max_depth=int(best['max_depth']),\n",
    "    min_samples_leaf=int(best['min_samples_leaf'])\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_ho_rf = model.predict(X_test)\n",
    "\n",
    "f1_ho_rf = metrics.f1_score(y_test, y_pred_ho_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score на Baseline для случайного леса равен 0.83\n",
      "F1-score на HyperOpt для случайного леса равен 0.84\n"
     ]
    }
   ],
   "source": [
    "#Сравним результат с BaseLine\n",
    "print('F1-score на Baseline для случайного леса равен {:.2f}'.format(f1_rf_base))\n",
    "print('F1-score на HyperOpt для случайного леса равен {:.2f}'.format(f1_ho_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_lr(trial):\n",
    "    penalty = trial.suggest_categorical(name='penalty', choices= ['l2', 'none']) # тип регуляризации\n",
    "    solver = trial.suggest_categorical(name = 'solver', choices= ['newton-cg', 'lbfgs', 'sag']) # алгоритм оптимизации\n",
    "    C = trial.suggest_float(name='C', low=0.01, high=1, step = 0.1) # уровень силы регурялизации\n",
    "    \n",
    "    model = linear_model.LogisticRegression(\n",
    "        penalty = penalty,\n",
    "        solver = solver,\n",
    "        C = C,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=5, scoring=\"f1\", n_jobs=-1).mean()\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-14 13:40:52,067]\u001b[0m A new study created in memory with name: LogisticRegression\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-14 13:40:59,861]\u001b[0m Trial 0 finished with value: 0.7744882223208942 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.51}. Best is trial 0 with value: 0.7744882223208942.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-14 13:41:07,306]\u001b[0m Trial 1 finished with value: 0.7690080349374169 and parameters: {'penalty': 'none', 'solver': 'sag', 'C': 0.31000000000000005}. Best is trial 0 with value: 0.7744882223208942.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-14 13:41:15,260]\u001b[0m Trial 2 finished with value: 0.7725256514893897 and parameters: {'penalty': 'l2', 'solver': 'newton-cg', 'C': 0.7100000000000001}. Best is trial 0 with value: 0.7744882223208942.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "\u001b[32m[I 2022-10-14 13:42:05,068]\u001b[0m Trial 3 finished with value: 0.7233491452286801 and parameters: {'penalty': 'none', 'solver': 'newton-cg', 'C': 0.91}. Best is trial 0 with value: 0.7744882223208942.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-14 13:42:13,092]\u001b[0m Trial 4 finished with value: 0.7799921548046879 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.31000000000000005}. Best is trial 4 with value: 0.7799921548046879.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-14 13:42:20,461]\u001b[0m Trial 5 finished with value: 0.7690080349374169 and parameters: {'penalty': 'none', 'solver': 'sag', 'C': 0.91}. Best is trial 4 with value: 0.7799921548046879.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-14 13:42:27,824]\u001b[0m Trial 6 finished with value: 0.7820005016970459 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.11}. Best is trial 6 with value: 0.7820005016970459.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "\u001b[32m[I 2022-10-14 13:43:17,381]\u001b[0m Trial 7 finished with value: 0.7233491452286801 and parameters: {'penalty': 'none', 'solver': 'newton-cg', 'C': 0.81}. Best is trial 6 with value: 0.7820005016970459.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-14 13:43:24,563]\u001b[0m Trial 8 finished with value: 0.7696711455789366 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.91}. Best is trial 6 with value: 0.7820005016970459.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2022-10-14 13:43:26,733]\u001b[0m Trial 9 finished with value: 0.7560783598917695 and parameters: {'penalty': 'none', 'solver': 'lbfgs', 'C': 0.11}. Best is trial 6 with value: 0.7820005016970459.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-14 13:43:28,244]\u001b[0m Trial 10 finished with value: 0.7749459870866194 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.01}. Best is trial 6 with value: 0.7820005016970459.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-14 13:43:35,452]\u001b[0m Trial 11 finished with value: 0.7799921548046879 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.31000000000000005}. Best is trial 6 with value: 0.7820005016970459.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-14 13:43:42,684]\u001b[0m Trial 12 finished with value: 0.7805104493934761 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.21000000000000002}. Best is trial 6 with value: 0.7820005016970459.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-14 13:43:49,884]\u001b[0m Trial 13 finished with value: 0.7820005016970459 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.11}. Best is trial 6 with value: 0.7820005016970459.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-14 13:43:55,834]\u001b[0m Trial 14 finished with value: 0.7743180436015331 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.01}. Best is trial 6 with value: 0.7820005016970459.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2022-10-14 13:43:58,119]\u001b[0m Trial 15 finished with value: 0.7737257662151599 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.51}. Best is trial 6 with value: 0.7820005016970459.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-14 13:44:05,299]\u001b[0m Trial 16 finished with value: 0.7820005016970459 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.11}. Best is trial 6 with value: 0.7820005016970459.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-14 13:44:12,294]\u001b[0m Trial 17 finished with value: 0.7778486011141373 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.41000000000000003}. Best is trial 6 with value: 0.7820005016970459.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2022-10-14 13:44:14,605]\u001b[0m Trial 18 finished with value: 0.7843446420739548 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.11}. Best is trial 18 with value: 0.7843446420739548.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2022-10-14 13:44:17,038]\u001b[0m Trial 19 finished with value: 0.7792866334316307 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.21000000000000002}. Best is trial 18 with value: 0.7843446420739548.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2022-10-14 13:44:19,286]\u001b[0m Trial 20 finished with value: 0.7719651519816895 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.6100000000000001}. Best is trial 18 with value: 0.7843446420739548.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2022-10-14 13:44:21,590]\u001b[0m Trial 21 finished with value: 0.7843446420739548 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.11}. Best is trial 18 with value: 0.7843446420739548.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-14 13:44:23,062]\u001b[0m Trial 22 finished with value: 0.7749459870866194 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.01}. Best is trial 18 with value: 0.7843446420739548.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2022-10-14 13:44:25,334]\u001b[0m Trial 23 finished with value: 0.7792866334316307 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.21000000000000002}. Best is trial 18 with value: 0.7843446420739548.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2022-10-14 13:44:27,582]\u001b[0m Trial 24 finished with value: 0.7843446420739548 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.11}. Best is trial 18 with value: 0.7843446420739548.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2022-10-14 13:44:29,713]\u001b[0m Trial 25 finished with value: 0.7792866334316307 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.21000000000000002}. Best is trial 18 with value: 0.7843446420739548.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2022-10-14 13:44:31,830]\u001b[0m Trial 26 finished with value: 0.7560783598917695 and parameters: {'penalty': 'none', 'solver': 'lbfgs', 'C': 0.41000000000000003}. Best is trial 18 with value: 0.7843446420739548.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-14 13:44:33,395]\u001b[0m Trial 27 finished with value: 0.7749459870866194 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.01}. Best is trial 18 with value: 0.7843446420739548.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2022-10-14 13:44:35,637]\u001b[0m Trial 28 finished with value: 0.7843446420739548 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.11}. Best is trial 18 with value: 0.7843446420739548.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2022-10-14 13:44:37,739]\u001b[0m Trial 29 finished with value: 0.779237413989353 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.31000000000000005}. Best is trial 18 with value: 0.7843446420739548.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2022-10-14 13:44:39,999]\u001b[0m Trial 30 finished with value: 0.7792866334316307 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.21000000000000002}. Best is trial 18 with value: 0.7843446420739548.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2022-10-14 13:44:42,143]\u001b[0m Trial 31 finished with value: 0.7843446420739548 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.11}. Best is trial 18 with value: 0.7843446420739548.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2022-10-14 13:44:44,390]\u001b[0m Trial 32 finished with value: 0.7843446420739548 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.11}. Best is trial 18 with value: 0.7843446420739548.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-14 13:44:45,852]\u001b[0m Trial 33 finished with value: 0.7749459870866194 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.01}. Best is trial 18 with value: 0.7843446420739548.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "\u001b[32m[I 2022-10-14 13:45:34,808]\u001b[0m Trial 34 finished with value: 0.7233491452286801 and parameters: {'penalty': 'none', 'solver': 'newton-cg', 'C': 0.21000000000000002}. Best is trial 18 with value: 0.7843446420739548.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2022-10-14 13:45:36,925]\u001b[0m Trial 35 finished with value: 0.7777418955251229 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.41000000000000003}. Best is trial 18 with value: 0.7843446420739548.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-14 13:45:43,309]\u001b[0m Trial 36 finished with value: 0.7794872072632637 and parameters: {'penalty': 'l2', 'solver': 'newton-cg', 'C': 0.31000000000000005}. Best is trial 18 with value: 0.7843446420739548.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2022-10-14 13:45:45,432]\u001b[0m Trial 37 finished with value: 0.7560783598917695 and parameters: {'penalty': 'none', 'solver': 'lbfgs', 'C': 0.11}. Best is trial 18 with value: 0.7843446420739548.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2022-10-14 13:45:47,659]\u001b[0m Trial 38 finished with value: 0.7719651519816895 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.6100000000000001}. Best is trial 18 with value: 0.7843446420739548.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-14 13:45:51,101]\u001b[0m Trial 39 finished with value: 0.7749459870866194 and parameters: {'penalty': 'l2', 'solver': 'newton-cg', 'C': 0.01}. Best is trial 18 with value: 0.7843446420739548.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2022-10-14 13:45:53,240]\u001b[0m Trial 40 finished with value: 0.7560783598917695 and parameters: {'penalty': 'none', 'solver': 'lbfgs', 'C': 0.31000000000000005}. Best is trial 18 with value: 0.7843446420739548.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2022-10-14 13:45:55,389]\u001b[0m Trial 41 finished with value: 0.7843446420739548 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.11}. Best is trial 18 with value: 0.7843446420739548.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2022-10-14 13:45:57,647]\u001b[0m Trial 42 finished with value: 0.7843446420739548 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.11}. Best is trial 18 with value: 0.7843446420739548.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2022-10-14 13:45:59,783]\u001b[0m Trial 43 finished with value: 0.7792866334316307 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.21000000000000002}. Best is trial 18 with value: 0.7843446420739548.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-14 13:46:01,149]\u001b[0m Trial 44 finished with value: 0.7749459870866194 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.01}. Best is trial 18 with value: 0.7843446420739548.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2022-10-14 13:46:03,300]\u001b[0m Trial 45 finished with value: 0.7843446420739548 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.11}. Best is trial 18 with value: 0.7843446420739548.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2022-10-14 13:46:05,561]\u001b[0m Trial 46 finished with value: 0.7843446420739548 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.11}. Best is trial 18 with value: 0.7843446420739548.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-10-14 13:46:11,477]\u001b[0m Trial 47 finished with value: 0.7803982420461868 and parameters: {'penalty': 'l2', 'solver': 'newton-cg', 'C': 0.21000000000000002}. Best is trial 18 with value: 0.7843446420739548.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2022-10-14 13:46:13,613]\u001b[0m Trial 48 finished with value: 0.7560783598917695 and parameters: {'penalty': 'none', 'solver': 'lbfgs', 'C': 0.31000000000000005}. Best is trial 18 with value: 0.7843446420739548.\u001b[0m\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.01, 1] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
      "  warnings.warn(\n",
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2022-10-14 13:46:15,797]\u001b[0m Trial 49 finished with value: 0.7716759100571287 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.81}. Best is trial 18 with value: 0.7843446420739548.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 6min 58s\n",
      "Wall time: 5min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "study = optuna.create_study(study_name='LogisticRegression', direction='maximize')\n",
    "\n",
    "study.optimize(optuna_lr, n_trials= 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\medol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model = linear_model.LogisticRegression(**study.best_params, random_state=random_state)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_optuna_lr = model.predict(X_test)\n",
    "\n",
    "f1_optuna_lr = metrics.f1_score(y_test, y_pred_optuna_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score на Baseline для логистической регресии равен 0.79\n",
      "F1-score на Optuna для логистической регресии равен 0.79\n"
     ]
    }
   ],
   "source": [
    "#Сравним результат с BaseLine\n",
    "print('F1-score на Baseline для логистической регресии равен {:.2f}'.format(f1_logReg_base))\n",
    "print('F1-score на Optuna для логистической регресии равен {:.2f}'.format(f1_optuna_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Случайный лес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_rf(trial):\n",
    "    n_estimators =  trial.suggest_int('n_estimators', 80, 200, 1)\n",
    "    max_depth = trial.suggest_int('max_depth', 1, 30, 1)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 5, 25, 1)\n",
    "    \n",
    "    model = ensemble.RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_leaf = min_samples_leaf,\n",
    "        random_state=random_state \n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    score = cross_val_score(model, X_train, y_train, cv = 5, scoring = 'f1', n_jobs= -1).mean()\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-14 13:46:16,422]\u001b[0m A new study created in memory with name: RandomForestClassification\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:46:19,058]\u001b[0m Trial 0 finished with value: 0.7848672562637755 and parameters: {'n_estimators': 134, 'max_depth': 10, 'min_samples_leaf': 10}. Best is trial 0 with value: 0.7848672562637755.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:46:21,617]\u001b[0m Trial 1 finished with value: 0.7815825502541451 and parameters: {'n_estimators': 136, 'max_depth': 22, 'min_samples_leaf': 17}. Best is trial 0 with value: 0.7848672562637755.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:46:24,963]\u001b[0m Trial 2 finished with value: 0.7865302782680568 and parameters: {'n_estimators': 164, 'max_depth': 14, 'min_samples_leaf': 11}. Best is trial 2 with value: 0.7865302782680568.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:46:27,503]\u001b[0m Trial 3 finished with value: 0.7732934983483443 and parameters: {'n_estimators': 149, 'max_depth': 29, 'min_samples_leaf': 21}. Best is trial 2 with value: 0.7865302782680568.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:46:29,273]\u001b[0m Trial 4 finished with value: 0.7680755900409602 and parameters: {'n_estimators': 92, 'max_depth': 18, 'min_samples_leaf': 22}. Best is trial 2 with value: 0.7865302782680568.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:46:33,512]\u001b[0m Trial 5 finished with value: 0.7896575864376845 and parameters: {'n_estimators': 192, 'max_depth': 16, 'min_samples_leaf': 11}. Best is trial 5 with value: 0.7896575864376845.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:46:36,902]\u001b[0m Trial 6 finished with value: 0.7721135526522647 and parameters: {'n_estimators': 179, 'max_depth': 7, 'min_samples_leaf': 7}. Best is trial 5 with value: 0.7896575864376845.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:46:39,539]\u001b[0m Trial 7 finished with value: 0.784881869977005 and parameters: {'n_estimators': 93, 'max_depth': 13, 'min_samples_leaf': 10}. Best is trial 5 with value: 0.7896575864376845.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:46:42,095]\u001b[0m Trial 8 finished with value: 0.7654882197428216 and parameters: {'n_estimators': 120, 'max_depth': 13, 'min_samples_leaf': 24}. Best is trial 5 with value: 0.7896575864376845.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:46:46,235]\u001b[0m Trial 9 finished with value: 0.7863207783231865 and parameters: {'n_estimators': 167, 'max_depth': 24, 'min_samples_leaf': 13}. Best is trial 5 with value: 0.7896575864376845.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:46:47,358]\u001b[0m Trial 10 finished with value: 0.7077415296224445 and parameters: {'n_estimators': 193, 'max_depth': 1, 'min_samples_leaf': 5}. Best is trial 5 with value: 0.7896575864376845.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:46:51,849]\u001b[0m Trial 11 finished with value: 0.7832699960116295 and parameters: {'n_estimators': 199, 'max_depth': 18, 'min_samples_leaf': 15}. Best is trial 5 with value: 0.7896575864376845.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:46:54,729]\u001b[0m Trial 12 finished with value: 0.7594462060071714 and parameters: {'n_estimators': 172, 'max_depth': 6, 'min_samples_leaf': 11}. Best is trial 5 with value: 0.7896575864376845.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:46:58,206]\u001b[0m Trial 13 finished with value: 0.7815588495056345 and parameters: {'n_estimators': 157, 'max_depth': 17, 'min_samples_leaf': 17}. Best is trial 5 with value: 0.7896575864376845.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:47:02,504]\u001b[0m Trial 14 finished with value: 0.7993645900213912 and parameters: {'n_estimators': 184, 'max_depth': 22, 'min_samples_leaf': 7}. Best is trial 14 with value: 0.7993645900213912.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:47:06,688]\u001b[0m Trial 15 finished with value: 0.8012033445926331 and parameters: {'n_estimators': 185, 'max_depth': 23, 'min_samples_leaf': 7}. Best is trial 15 with value: 0.8012033445926331.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:47:10,680]\u001b[0m Trial 16 finished with value: 0.8007225806911199 and parameters: {'n_estimators': 185, 'max_depth': 30, 'min_samples_leaf': 7}. Best is trial 15 with value: 0.8012033445926331.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:47:13,505]\u001b[0m Trial 17 finished with value: 0.7990717317545416 and parameters: {'n_estimators': 116, 'max_depth': 30, 'min_samples_leaf': 5}. Best is trial 15 with value: 0.8012033445926331.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:47:16,750]\u001b[0m Trial 18 finished with value: 0.7966248900302216 and parameters: {'n_estimators': 151, 'max_depth': 26, 'min_samples_leaf': 8}. Best is trial 15 with value: 0.8012033445926331.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:47:20,500]\u001b[0m Trial 19 finished with value: 0.7970916247817957 and parameters: {'n_estimators': 180, 'max_depth': 27, 'min_samples_leaf': 8}. Best is trial 15 with value: 0.8012033445926331.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:47:24,157]\u001b[0m Trial 20 finished with value: 0.7897236678987033 and parameters: {'n_estimators': 199, 'max_depth': 21, 'min_samples_leaf': 14}. Best is trial 15 with value: 0.8012033445926331.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:47:28,214]\u001b[0m Trial 21 finished with value: 0.7995990033439341 and parameters: {'n_estimators': 182, 'max_depth': 22, 'min_samples_leaf': 7}. Best is trial 15 with value: 0.8012033445926331.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:47:32,587]\u001b[0m Trial 22 finished with value: 0.803229786441172 and parameters: {'n_estimators': 185, 'max_depth': 26, 'min_samples_leaf': 5}. Best is trial 22 with value: 0.803229786441172.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:47:36,612]\u001b[0m Trial 23 finished with value: 0.8013610667172006 and parameters: {'n_estimators': 169, 'max_depth': 27, 'min_samples_leaf': 5}. Best is trial 22 with value: 0.803229786441172.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:47:40,516]\u001b[0m Trial 24 finished with value: 0.8014851218285161 and parameters: {'n_estimators': 169, 'max_depth': 25, 'min_samples_leaf': 5}. Best is trial 22 with value: 0.803229786441172.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:47:44,423]\u001b[0m Trial 25 finished with value: 0.800614532272361 and parameters: {'n_estimators': 162, 'max_depth': 26, 'min_samples_leaf': 5}. Best is trial 22 with value: 0.803229786441172.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:47:47,804]\u001b[0m Trial 26 finished with value: 0.8014811813337108 and parameters: {'n_estimators': 143, 'max_depth': 27, 'min_samples_leaf': 5}. Best is trial 22 with value: 0.803229786441172.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:47:50,630]\u001b[0m Trial 27 finished with value: 0.7956997102154104 and parameters: {'n_estimators': 127, 'max_depth': 19, 'min_samples_leaf': 9}. Best is trial 22 with value: 0.803229786441172.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:47:53,201]\u001b[0m Trial 28 finished with value: 0.7821316243586058 and parameters: {'n_estimators': 146, 'max_depth': 25, 'min_samples_leaf': 18}. Best is trial 22 with value: 0.803229786441172.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:47:56,156]\u001b[0m Trial 29 finished with value: 0.7925018441340035 and parameters: {'n_estimators': 141, 'max_depth': 28, 'min_samples_leaf': 9}. Best is trial 22 with value: 0.803229786441172.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:47:58,471]\u001b[0m Trial 30 finished with value: 0.7841533724716031 and parameters: {'n_estimators': 110, 'max_depth': 24, 'min_samples_leaf': 13}. Best is trial 22 with value: 0.803229786441172.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:48:02,502]\u001b[0m Trial 31 finished with value: 0.8013597386081548 and parameters: {'n_estimators': 173, 'max_depth': 28, 'min_samples_leaf': 5}. Best is trial 22 with value: 0.803229786441172.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:48:06,117]\u001b[0m Trial 32 finished with value: 0.7975780110197871 and parameters: {'n_estimators': 156, 'max_depth': 26, 'min_samples_leaf': 6}. Best is trial 22 with value: 0.803229786441172.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:48:09,214]\u001b[0m Trial 33 finished with value: 0.7959266056781249 and parameters: {'n_estimators': 134, 'max_depth': 20, 'min_samples_leaf': 6}. Best is trial 22 with value: 0.803229786441172.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:48:12,829]\u001b[0m Trial 34 finished with value: 0.7937778891620205 and parameters: {'n_estimators': 170, 'max_depth': 28, 'min_samples_leaf': 9}. Best is trial 22 with value: 0.803229786441172.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:48:16,488]\u001b[0m Trial 35 finished with value: 0.7980817230390201 and parameters: {'n_estimators': 162, 'max_depth': 24, 'min_samples_leaf': 6}. Best is trial 22 with value: 0.803229786441172.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:48:19,850]\u001b[0m Trial 36 finished with value: 0.8019755796179373 and parameters: {'n_estimators': 142, 'max_depth': 29, 'min_samples_leaf': 5}. Best is trial 22 with value: 0.803229786441172.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:48:22,358]\u001b[0m Trial 37 finished with value: 0.7758935201348767 and parameters: {'n_estimators': 141, 'max_depth': 30, 'min_samples_leaf': 20}. Best is trial 22 with value: 0.803229786441172.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:48:25,070]\u001b[0m Trial 38 finished with value: 0.7901082961336404 and parameters: {'n_estimators': 130, 'max_depth': 29, 'min_samples_leaf': 10}. Best is trial 22 with value: 0.803229786441172.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:48:26,680]\u001b[0m Trial 39 finished with value: 0.78177612929469 and parameters: {'n_estimators': 80, 'max_depth': 10, 'min_samples_leaf': 8}. Best is trial 22 with value: 0.803229786441172.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:48:29,672]\u001b[0m Trial 40 finished with value: 0.7884946998678946 and parameters: {'n_estimators': 148, 'max_depth': 25, 'min_samples_leaf': 12}. Best is trial 22 with value: 0.803229786441172.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:48:33,338]\u001b[0m Trial 41 finished with value: 0.8007705537082295 and parameters: {'n_estimators': 156, 'max_depth': 28, 'min_samples_leaf': 5}. Best is trial 22 with value: 0.803229786441172.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:48:37,347]\u001b[0m Trial 42 finished with value: 0.796317261645668 and parameters: {'n_estimators': 176, 'max_depth': 27, 'min_samples_leaf': 6}. Best is trial 22 with value: 0.803229786441172.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:48:41,055]\u001b[0m Trial 43 finished with value: 0.7971915490987459 and parameters: {'n_estimators': 166, 'max_depth': 25, 'min_samples_leaf': 6}. Best is trial 22 with value: 0.803229786441172.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:48:43,224]\u001b[0m Trial 44 finished with value: 0.7705451017161722 and parameters: {'n_estimators': 125, 'max_depth': 23, 'min_samples_leaf': 25}. Best is trial 22 with value: 0.803229786441172.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:48:47,661]\u001b[0m Trial 45 finished with value: 0.8024745588961839 and parameters: {'n_estimators': 194, 'max_depth': 27, 'min_samples_leaf': 5}. Best is trial 22 with value: 0.803229786441172.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:48:51,876]\u001b[0m Trial 46 finished with value: 0.7989035153102164 and parameters: {'n_estimators': 189, 'max_depth': 15, 'min_samples_leaf': 6}. Best is trial 22 with value: 0.803229786441172.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:48:55,817]\u001b[0m Trial 47 finished with value: 0.7984322206784923 and parameters: {'n_estimators': 190, 'max_depth': 29, 'min_samples_leaf': 8}. Best is trial 22 with value: 0.803229786441172.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:48:59,653]\u001b[0m Trial 48 finished with value: 0.7898347395300584 and parameters: {'n_estimators': 195, 'max_depth': 21, 'min_samples_leaf': 10}. Best is trial 22 with value: 0.803229786441172.\u001b[0m\n",
      "\u001b[32m[I 2022-10-14 13:49:00,393]\u001b[0m Trial 49 finished with value: 0.7110062213693763 and parameters: {'n_estimators': 106, 'max_depth': 1, 'min_samples_leaf': 7}. Best is trial 22 with value: 0.803229786441172.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 11s\n",
      "Wall time: 2min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "study = optuna.create_study(study_name='RandomForestClassification', direction='maximize')\n",
    "\n",
    "study.optimize(optuna_rf, n_trials= 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ensemble.RandomForestClassifier(**study.best_params, random_state=random_state)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_optuna_rf = model.predict(X_test)\n",
    "\n",
    "f1_optuna_rf = metrics.f1_score(y_test, y_pred_optuna_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score на Baseline для Случайного леса равен 0.83\n",
      "F1-score на Optuna для Случайного леса равен 0.83\n"
     ]
    }
   ],
   "source": [
    "#Сравним результат с BaseLine\n",
    "print('F1-score на Baseline для Случайного леса равен {:.2f}'.format(f1_rf_base))\n",
    "print('F1-score на Optuna для Случайного леса равен {:.2f}'.format(f1_optuna_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итоги"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score на Baseline для логистической регресии равен 0.79\n",
      "F1-score на GridSearchCV для логистической регресии равен 0.79\n",
      "F1-score на RandomizedSrearchCV для логистической регресии равен 0.80\n",
      "F1-score на HyperOpt для линейной регресии равен 0.80\n",
      "F1-score на Optuna для логистической регресии равен 0.79\n"
     ]
    }
   ],
   "source": [
    "print('F1-score на Baseline для логистической регресии равен {:.2f}'.format(f1_logReg_base))\n",
    "print('F1-score на GridSearchCV для логистической регресии равен {:.2f}'.format(f1_gs_logReg))\n",
    "print('F1-score на RandomizedSrearchCV для логистической регресии равен {:.2f}'.format(f1_rs_logReg))\n",
    "print('F1-score на HyperOpt для логистической регресии равен {:.2f}'.format(f1_ho_lr))\n",
    "print('F1-score на Optuna для логистической регресии равен {:.2f}'.format(f1_optuna_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Случайный лес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score на Baseline для Случайного леса равен 0.83\n",
      "F1-score на GridResearchCV для случайного леса равен 0.83\n",
      "F1-score на RandomizedSrearchCV для случайного леса равен 0.83\n",
      "F1-score на HyperOpt для случайного леса равен 0.84\n",
      "F1-score на Optuna для Случайного леса равен 0.83\n"
     ]
    }
   ],
   "source": [
    "print('F1-score на Baseline для Случайного леса равен {:.2f}'.format(f1_rf_base))\n",
    "print('F1-score на GridResearchCV для случайного леса равен {:.2f}'.format(f1_gs_rf))\n",
    "print('F1-score на RandomizedSrearchCV для случайного леса равен {:.2f}'.format(f1_rs_rf))\n",
    "print('F1-score на HyperOpt для случайного леса равен {:.2f}'.format(f1_ho_rf))\n",
    "print('F1-score на Optuna для Случайного леса равен {:.2f}'.format(f1_optuna_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для логистической регрессии лучший результат f1_score = 0.8 показал RandomSearchCV и HyperOpt, а для Случайного леса лучший результат f1_score = 0.84 показал HyperOpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S. В моём личном рейтинге страданий:\n",
    "\n",
    "1 Место - HyperOpt, который никак не хотел вбирать в себя параметры логистической регресии\\\n",
    "2 Место - GridResearchCV, который на большой сетке параметров забрал у меня ноутбук на 2.5 часа и пришлось перенастраивать сетку\\\n",
    "3 Место - Optuna, которая, к сожалению, не преодолеть BaseLine, а я так в нее верил"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "692452926a1002f011508fca2f08b574b400eacdc3255d89f24e5bc9f497edaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
